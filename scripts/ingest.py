"""
Global Observer - Telegram Ingestion Pipeline
Deterministic (No AI) content extraction from Telegram channels
"""

import os
import re
import json
import asyncio
from datetime import datetime, timezone
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass
from telethon import TelegramClient
from telethon.tl.types import Message
from supabase import create_client, Client

# Configuration
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY", "")
TELEGRAM_API_ID = os.environ.get("TELEGRAM_API_ID", "")
TELEGRAM_API_HASH = os.environ.get("TELEGRAM_API_HASH", "")

# OSINT Channels to monitor (public channels)
CHANNELS = [
    "ukrainenow",
    "truexanewsua",
    "operativnoZSU",
    "voyaborbu",
    "DeepStateUA",
]

@dataclass
class ExtractedData:
    """Extracted information from a message"""
    category: Optional[str]
    location_name: Optional[str]
    coordinates: Optional[Tuple[float, float]]
    confidence: float
    keywords_matched: List[str]


class DeterministicExtractor:
    """
    Rule-based content extraction without AI.
    Uses keyword matching, regex patterns, and gazetteer lookup.
    """
    
    # Category patterns (German/English/Ukrainian/Russian keywords)
    CATEGORY_PATTERNS = {
        'shelling': re.compile(
            r'(beschuss|Ğ°Ñ€Ñ‚Ğ¸Ğ»ĞµÑ€|shell|Ğ¾Ğ±ÑÑ‚Ñ€Ñ–Ğ»|ÑƒĞ´Ğ°Ñ€|Ñ€Ğ°ĞºĞµÑ‚|missile|rocket|'
            r'ÑĞ½Ğ°Ñ€ÑĞ´|Ğ¼Ñ–Ğ½Ğ¾Ğ¼ĞµÑ‚Ğ½|mortar|grad|mÃ¶rser|artiller|Ğ Ğ¡Ğ—Ğ’|MLRS)',
            re.IGNORECASE
        ),
        'air_raid': re.compile(
            r'(luftalarm|Ğ¿Ğ¾Ğ²Ñ–Ñ‚Ñ€ÑĞ½Ğ° Ñ‚Ñ€Ğ¸Ğ²Ğ¾Ğ³Ğ°|air\s*raid|ÑĞ¸Ñ€ĞµĞ½|alarm|'
            r'Ğ°Ğ²Ñ–Ğ°Ñ†Ñ–Ğ¹Ğ½|Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ°Ñ Ñ‚Ñ€ĞµĞ²Ğ¾Ğ³Ğ°|air\s*alert)',
            re.IGNORECASE
        ),
        'drone': re.compile(
            r'(drohn|drone|Ğ‘ĞŸĞ›Ğ|shahed|Ğ³ĞµÑ€Ğ°Ğ½ÑŒ|geran|UAV|'
            r'Ğ±ĞµĞ·Ğ¿Ñ–Ğ»Ğ¾Ñ‚Ğ½|Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½|orlan|lancet)',
            re.IGNORECASE
        ),
        'combat': re.compile(
            r'(gefecht|Ğ±Ñ–Ğ¹|combat|fight|clash|assault|storm|'
            r'ÑˆÑ‚ÑƒÑ€Ğ¼|Ğ½Ğ°ÑÑ‚ÑƒĞ¿|offensive|Ğ°Ñ‚Ğ°ĞºĞ°|attack)',
            re.IGNORECASE
        ),
        'movement': re.compile(
            r'(bewegung|Ñ€ÑƒÑ…|movement|ĞºĞ¾Ğ»Ğ¾Ğ½|convoy|panzer|tank|'
            r'Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½|Ñ‚ĞµÑ…Ğ½Ñ–ĞºĞ°|equipment|advance|retreat)',
            re.IGNORECASE
        ),
        'naval': re.compile(
            r'(marine|Ñ„Ğ»Ğ¾Ñ‚|naval|ship|schiff|ĞºĞ¾Ñ€Ğ°Ğ±Ğ»|Ñ‡Ğ¾Ñ€Ğ½Ğµ Ğ¼Ğ¾Ñ€Ğµ|'
            r'black sea|schwarzes meer|Ğ¿Ñ–Ğ´Ğ²Ğ¾Ğ´Ğ½|submarine)',
            re.IGNORECASE
        ),
        'infrastructure': re.compile(
            r'(infrastruktur|Ñ–Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€|infrastructure|ÑĞ½ĞµÑ€Ğ³Ğ¾|'
            r'elektrizitÃ¤t|electricity|power|station|Ğ¿Ñ–Ğ´ÑÑ‚Ğ°Ğ½Ñ†|'
            r'Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€|transformer)',
            re.IGNORECASE
        ),
        'humanitarian': re.compile(
            r'(humanitÃ¤r|Ğ³ÑƒĞ¼Ğ°Ğ½Ñ–Ñ‚Ğ°Ñ€|humanitarian|ĞµĞ²Ğ°ĞºÑƒĞ°Ñ†|'
            r'evacuation|refugees|flÃ¼chtling|Ğ±Ñ–Ğ¶ĞµĞ½Ñ†|civilians)',
            re.IGNORECASE
        ),
        'political': re.compile(
            r'(politik|Ğ¿Ğ¾Ğ»Ñ–Ñ‚Ğ¸Ğº|political|diplomati|president|'
            r'minister|ÑĞ°Ğ½ĞºÑ†|sanction|Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€|negotiation)',
            re.IGNORECASE
        ),
    }
    
    # Severity indicators
    SEVERITY_PATTERNS = {
        'critical': re.compile(
            r'(massiv|Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½|Ğ¼Ğ°ÑĞ¸Ğ²Ğ½|heavy|schwer|significant|'
            r'multiple|Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾|mehrere|killed|Ğ·Ğ°Ğ³Ğ¸Ğ±Ğ»|Ğ¿Ğ¾Ğ³Ğ¸Ğ±Ğ»|casualties)',
            re.IGNORECASE
        ),
        'high': re.compile(
            r'(intense|intensiv|Ñ–Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½|damage|Ğ¿Ğ¾ÑˆĞºĞ¾Ğ´Ğ¶ĞµĞ½|'
            r'destroy|Ğ·Ğ½Ğ¸Ñ‰ĞµĞ½|wounded|Ğ¿Ğ¾Ñ€Ğ°Ğ½ĞµĞ½)',
            re.IGNORECASE
        ),
        'medium': re.compile(
            r'(report|Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»Ñ|ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚|observed|ÑĞ¿Ğ¾ÑÑ‚ĞµÑ€Ñ–Ğ³Ğ°)',
            re.IGNORECASE
        ),
    }
    
    # Location gazetteer (major cities - expand as needed)
    LOCATIONS_GAZETTEER = {
        # Ukrainian cities with coordinates [lat, lng]
        'kyiv': (50.4501, 30.5234),
        'kiev': (50.4501, 30.5234),
        'ĞºĞ¸Ñ—Ğ²': (50.4501, 30.5234),
        'kharkiv': (49.9935, 36.2304),
        'Ñ…Ğ°Ñ€ĞºÑ–Ğ²': (49.9935, 36.2304),
        'Ñ…Ğ°Ñ€ÑŒĞºĞ¾Ğ²': (49.9935, 36.2304),
        'charkiw': (49.9935, 36.2304),
        'odesa': (46.4825, 30.7233),
        'odessa': (46.4825, 30.7233),
        'Ğ¾Ğ´ĞµÑĞ°': (46.4825, 30.7233),
        'dnipro': (48.4647, 35.0462),
        'Ğ´Ğ½Ñ–Ğ¿Ñ€Ğ¾': (48.4647, 35.0462),
        'zaporizhzhia': (47.8388, 35.1396),
        'Ğ·Ğ°Ğ¿Ğ¾Ñ€Ñ–Ğ¶Ğ¶Ñ': (47.8388, 35.1396),
        'lviv': (49.8397, 24.0297),
        'Ğ»ÑŒĞ²Ñ–Ğ²': (49.8397, 24.0297),
        'lemberg': (49.8397, 24.0297),
        'mariupol': (47.0958, 37.5494),
        'Ğ¼Ğ°Ñ€Ñ–ÑƒĞ¿Ğ¾Ğ»ÑŒ': (47.0958, 37.5494),
        'bakhmut': (48.5953, 38.0009),
        'Ğ±Ğ°Ñ…Ğ¼ÑƒÑ‚': (48.5953, 38.0009),
        'artemivsk': (48.5953, 38.0009),
        'kherson': (46.6354, 32.6169),
        'Ñ…ĞµÑ€ÑĞ¾Ğ½': (46.6354, 32.6169),
        'mykolaiv': (46.9750, 31.9946),
        'Ğ¼Ğ¸ĞºĞ¾Ğ»Ğ°Ñ—Ğ²': (46.9750, 31.9946),
        'sumy': (50.9077, 34.7981),
        'ÑÑƒĞ¼Ğ¸': (50.9077, 34.7981),
        'chernihiv': (51.4982, 31.2893),
        'Ñ‡ĞµÑ€Ğ½Ñ–Ğ³Ñ–Ğ²': (51.4982, 31.2893),
        'donetsk': (48.0159, 37.8028),
        'Ğ´Ğ¾Ğ½ĞµÑ†ÑŒĞº': (48.0159, 37.8028),
        'luhansk': (48.5740, 39.3078),
        'Ğ»ÑƒĞ³Ğ°Ğ½ÑÑŒĞº': (48.5740, 39.3078),
        'sevastopol': (44.6054, 33.5220),
        'ÑĞµĞ²Ğ°ÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»ÑŒ': (44.6054, 33.5220),
        'simferopol': (44.9521, 34.1024),
        'ÑÑ–Ğ¼Ñ„ĞµÑ€Ğ¾Ğ¿Ğ¾Ğ»ÑŒ': (44.9521, 34.1024),
        'crimea': (44.9521, 34.1024),
        'ĞºÑ€Ğ¸Ğ¼': (44.9521, 34.1024),
        'avdiivka': (48.1389, 37.7494),
        'Ğ°Ğ²Ğ´Ñ–Ñ—Ğ²ĞºĞ°': (48.1389, 37.7494),
        'kupyansk': (49.7078, 37.6178),
        'ĞºÑƒĞ¿\'ÑĞ½ÑÑŒĞº': (49.7078, 37.6178),
        'izium': (49.2108, 37.2547),
        'Ñ–Ğ·ÑĞ¼': (49.2108, 37.2547),
    }
    
    def extract(self, text: str) -> ExtractedData:
        """
        Extract structured data from message text using deterministic rules.
        """
        text_lower = text.lower()
        keywords_matched = []
        
        # 1. Determine category
        category = None
        max_matches = 0
        for cat, pattern in self.CATEGORY_PATTERNS.items():
            matches = pattern.findall(text)
            if len(matches) > max_matches:
                max_matches = len(matches)
                category = cat
                keywords_matched.extend(matches[:3])  # Keep top 3 matches
        
        # 2. Extract location
        location_name = None
        coordinates = None
        for loc_name, coords in self.LOCATIONS_GAZETTEER.items():
            if loc_name in text_lower:
                location_name = loc_name.title()
                coordinates = coords
                keywords_matched.append(loc_name)
                break
        
        # 3. Calculate confidence score (0-1)
        confidence = 0.0
        if category:
            confidence += 0.4
        if location_name:
            confidence += 0.4
        if max_matches > 1:
            confidence += 0.1
        if len(text) > 100:  # Longer messages tend to be more informative
            confidence += 0.1
        
        return ExtractedData(
            category=category,
            location_name=location_name,
            coordinates=coordinates,
            confidence=min(confidence, 1.0),
            keywords_matched=keywords_matched[:5]
        )
    
    def determine_severity(self, text: str) -> str:
        """Determine severity level from text"""
        for severity, pattern in self.SEVERITY_PATTERNS.items():
            if pattern.search(text):
                return severity
        return 'medium'


class IngestionPipeline:
    """
    Main ingestion pipeline for Telegram data.
    """
    
    def __init__(self):
        self.supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.extractor = DeterministicExtractor()
        self.telegram: Optional[TelegramClient] = None
    
    async def connect_telegram(self):
        """Initialize Telegram client"""
        self.telegram = TelegramClient(
            'global_observer_session',
            int(TELEGRAM_API_ID),
            TELEGRAM_API_HASH
        )
        await self.telegram.start()
        print("âœ… Connected to Telegram")
    
    async def disconnect_telegram(self):
        """Disconnect Telegram client"""
        if self.telegram:
            await self.telegram.disconnect()
    
    async def fetch_channel_messages(
        self, 
        channel: str, 
        limit: int = 100
    ) -> List[Message]:
        """Fetch recent messages from a channel"""
        messages = []
        try:
            async for message in self.telegram.iter_messages(channel, limit=limit):
                if message.text:  # Only text messages
                    messages.append(message)
        except Exception as e:
            print(f"âŒ Error fetching from {channel}: {e}")
        return messages
    
    def store_raw_report(
        self, 
        message: Message, 
        channel: str,
        extracted: ExtractedData
    ) -> Optional[str]:
        """Store raw message in database"""
        try:
            # Prepare coordinates
            coords = None
            if extracted.coordinates:
                coords = f"POINT({extracted.coordinates[1]} {extracted.coordinates[0]})"
            
            data = {
                "source": "telegram",
                "source_channel": channel,
                "source_url": f"https://t.me/{channel}/{message.id}",
                "source_message_id": str(message.id),
                "content": message.text[:2000],  # Limit length
                "extracted_category": extracted.category,
                "extracted_location": extracted.location_name,
                "confidence_score": extracted.confidence,
                "raw_data": {
                    "keywords": extracted.keywords_matched,
                    "message_date": message.date.isoformat() if message.date else None,
                    "views": message.views,
                }
            }
            
            result = self.supabase.table("raw_reports").insert(data).execute()
            return result.data[0]["id"] if result.data else None
            
        except Exception as e:
            print(f"âŒ Error storing raw report: {e}")
            return None
    
    def create_event_from_report(
        self, 
        report_id: str, 
        extracted: ExtractedData,
        message_text: str,
        message_date: datetime,
        source_url: str
    ) -> Optional[str]:
        """Create a verified event from an extracted report"""
        if not extracted.category or not extracted.coordinates:
            return None
        
        try:
            severity = self.extractor.determine_severity(message_text)
            
            # Create title from first sentence or first 100 chars
            title = message_text.split('.')[0][:100]
            if len(title) < len(message_text):
                title += "..."
            
            data = {
                "event_date": message_date.isoformat(),
                "title": title,
                "description": message_text[:500],
                "category": extracted.category,
                "severity": severity,
                "latitude": extracted.coordinates[0],
                "longitude": extracted.coordinates[1],
                "location_name": extracted.location_name,
                "source_url": source_url,
                "source_type": "telegram",
                "raw_report_id": report_id,
                "verified": False,  # Needs manual verification
                "tags": extracted.keywords_matched,
            }
            
            result = self.supabase.table("events").insert(data).execute()
            return result.data[0]["id"] if result.data else None
            
        except Exception as e:
            print(f"âŒ Error creating event: {e}")
            return None
    
    async def process_channel(self, channel: str, limit: int = 50):
        """Process messages from a single channel"""
        print(f"\nğŸ“¡ Processing channel: {channel}")
        messages = await self.fetch_channel_messages(channel, limit)
        
        stats = {"total": 0, "stored": 0, "events": 0, "skipped": 0}
        
        for message in messages:
            stats["total"] += 1
            
            # Check if already processed
            existing = self.supabase.table("raw_reports").select("id").eq(
                "source_message_id", str(message.id)
            ).eq("source_channel", channel).execute()
            
            if existing.data:
                stats["skipped"] += 1
                continue
            
            # Extract data
            extracted = self.extractor.extract(message.text)
            
            # Only store if we found something relevant
            if extracted.confidence < 0.3:
                stats["skipped"] += 1
                continue
            
            # Store raw report
            report_id = self.store_raw_report(message, channel, extracted)
            if report_id:
                stats["stored"] += 1
                
                # Auto-create event if high confidence
                if extracted.confidence >= 0.7 and extracted.coordinates:
                    event_id = self.create_event_from_report(
                        report_id,
                        extracted,
                        message.text,
                        message.date or datetime.now(timezone.utc),
                        f"https://t.me/{channel}/{message.id}"
                    )
                    if event_id:
                        stats["events"] += 1
        
        print(f"   ğŸ“Š Stats: {stats}")
        return stats
    
    async def run(self, channels: List[str] = None, limit: int = 50):
        """Run the full ingestion pipeline"""
        channels = channels or CHANNELS
        
        print("ğŸš€ Starting Global Observer Ingestion Pipeline")
        print(f"   Mode: Deterministic (No AI)")
        print(f"   Channels: {len(channels)}")
        
        await self.connect_telegram()
        
        total_stats = {"total": 0, "stored": 0, "events": 0, "skipped": 0}
        
        for channel in channels:
            try:
                stats = await self.process_channel(channel, limit)
                for key in total_stats:
                    total_stats[key] += stats[key]
            except Exception as e:
                print(f"âŒ Error processing {channel}: {e}")
        
        await self.disconnect_telegram()
        
        print("\n" + "="*50)
        print("ğŸ“ˆ PIPELINE COMPLETE")
        print(f"   Total messages: {total_stats['total']}")
        print(f"   Stored reports: {total_stats['stored']}")
        print(f"   Events created: {total_stats['events']}")
        print(f"   Skipped: {total_stats['skipped']}")
        print("="*50)


# Entry point for GitHub Actions
async def main():
    pipeline = IngestionPipeline()
    await pipeline.run()


if __name__ == "__main__":
    asyncio.run(main())
